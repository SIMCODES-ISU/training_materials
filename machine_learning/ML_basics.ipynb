{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "GuIVXkOyu_R7"
      },
      "outputs": [],
      "source": [
        "# 1. basic string operations\n",
        "sentence = \"Iowa State University, located in Ames, is a renowned public research university.\"\n",
        "print(\"[example sentence]: \" + sentence)\n",
        "print()\n",
        "\n",
        "# 1.1 convert to uppercase/lowercase\n",
        "uppercase = sentence.upper()\n",
        "lowercase = sentence.lower()\n",
        "print(\"Uppercase:\", uppercase)\n",
        "print(\"Lowercase:\", lowercase)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 1.2 split into words & join words\n",
        "words = sentence.split()\n",
        "print(\"Words in the sentence:\", words)\n",
        "print()\n",
        "\n",
        "joined_sentence = \" \".join(words)\n",
        "print(\"Joined Sentence:\", joined_sentence)\n",
        "print()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bHxK1YFXu_R9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 1.3 find substrings & replace substrings\n",
        "index = sentence.find(\"Ames\")\n",
        "# returns the index of the first occurrence of the substring.\n",
        "# if the substring is not found, it returns -1\n",
        "print(f\"'Ames' found at index: {index}\")\n",
        "print()\n",
        "\n",
        "modified_sentence = sentence.replace(\"Ames\", \"Ames, Iowa\")\n",
        "print(\"Modified Sentence:\", modified_sentence)\n",
        "print()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UUSOPddwu_R9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 1.4 access characters by Index\n",
        "first_char = sentence[0]\n",
        "last_char = sentence[-1]\n",
        "print(f\"First Character: {first_char}\")\n",
        "print(f\"Last Character: {last_char}\")\n",
        "print()\n",
        "\n",
        "substring = sentence[0:21]  # \"Iowa State University\", the blank space also counts\n",
        "print(\"Substring (0:21):\", substring)\n",
        "print()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zewlHdK8u_R9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2. NLTK. \"The Natural Language Toolkit\"\n",
        "import nltk\n",
        "nltk.download('punkt_tab')  # ensure tokenizer resources are available\n",
        "\n",
        "sentence = \"Iowa State University, located in Ames, is a renowned public research university.\"\n",
        "\n",
        "# 2.1 Tokenization\n",
        "# Word\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(sentence)\n",
        "print(\"Word Tokens:\", tokens)\n",
        "print()\n",
        "\n",
        "# Sentence\n",
        "from nltk.tokenize import sent_tokenize\n",
        "long_introduction = \"Iowa State University (ISU), in Ames, Iowa, is a top public research institution founded in 1858. \\\n",
        "Renowned for science, engineering, and agriculture, it hosts the U.S. Department of Energyâ€™s Ames Laboratory. \\\n",
        "With over 36,000 students, ISU fosters innovation and global impact.\"\n",
        "sentences = sent_tokenize(long_introduction)\n",
        "print(\"Sentence Tokens:\", sentences)\n",
        "print()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "r2JG-hjpu_R9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2.2 Stop Words Removal & Frequency Distribution\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"NLTK stopwords:\", stop_words)\n",
        "print()\n",
        "filtered_words = [word for word in word_tokenize(sentence) if word.lower() not in stop_words]\n",
        "print(\"Filtered Words (No Stop Words):\", filtered_words)\n",
        "print()\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "filtered_words = [word.lower() for word in filtered_words]\n",
        "freq_dist = FreqDist(filtered_words)\n",
        "print(\"Frequency Distribution:\")\n",
        "print(freq_dist.most_common(5))\n",
        "print()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "XIa7ARpou_R-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 3. Regular Expression (Regex)\n",
        "import re\n",
        "\n",
        "# 3.1 Check if a Pattern Exists\n",
        "pattern = r\"Ames\"\n",
        "match = re.search(pattern, sentence)  # stops after finding the first match in the string\n",
        "if match:\n",
        "    print(f\"Pattern '{pattern}' found at position: {match.start()}\")\n",
        "else:\n",
        "    print(f\"Pattern '{pattern}' not found\")\n",
        "print()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bNYPSQhgu_SA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 3.2 Find all Case-Insensitive Matching\n",
        "pattern = r\"university\"\n",
        "matches = re.findall(pattern, sentence, re.IGNORECASE)\n",
        "print(f\"Case-insensitive matches for '{pattern}':\", matches)\n",
        "print()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "G_MXBM3iu_SA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 3.3 Split String Using a Pattern\n",
        "pattern = r\",|\\.\"  # The pipe symbol \"|\" means \"or\" in regex; \"\\.\" matches the dot \".\"\n",
        "parts = re.split(pattern, sentence)\n",
        "print(\"Split Sentence:\", parts)\n",
        "print()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Q5OdrG6Ku_SA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 3.4 Validate Patterns (e.g., Email-Like Text)\n",
        "test_string = \"Contact me at qli@iastate.edu\"\n",
        "\n",
        "pattern = r\"\\b[A-Za-z0-9._-]+@[A-Za-z0-9._-]+\\.[A-Za-z]{2,}\\b\"\n",
        "# \\b\n",
        "# Matches a word boundary, ensuring the email address is a standalone word and not part of a larger string.\n",
        "# This is used at both the start and end of the pattern.\n",
        "\n",
        "# [A-Za-z0-9._-]+\n",
        "# Matches the local part of the email address (before the @).\n",
        "# Allows any combination of uppercase and lowercase letters (A-Za-z), digits (0-9), dots (.), underscores (_), and dashes (-).\n",
        "# The \"+\" ensures there is at least one character.\n",
        "\n",
        "# [A-Za-z]{2,}\n",
        "# Matches the top-level domain (e.g., com, org, net).\n",
        "# Accepts at least two characters ({2,}) and ensures they are only uppercase (A-Z) or lowercase (a-z) letters.\n",
        "\n",
        "if re.search(pattern, test_string):\n",
        "    print(\"Valid email found!\")\n",
        "else:\n",
        "    print(\"No valid email found.\")\n",
        "print()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qgyjE0a0u_SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. scikit-learn. \"Machine Learning in Python\"\n",
        "# 4.1 classification\n",
        "# Authors: The scikit-learn developers\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "from sklearn.datasets import make_circles, make_classification, make_moons\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "names = [\n",
        "    \"Nearest Neighbors\",\n",
        "    \"Linear SVM\",\n",
        "    \"Decision Tree\",\n",
        "    \"Random Forest\",\n",
        "    \"Neural Net\",\n",
        "    \"Naive Bayes\",\n",
        "]\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(kernel=\"linear\", C=0.025, random_state=42),\n",
        "    DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    RandomForestClassifier(\n",
        "        max_depth=5, n_estimators=10, max_features=1, random_state=42\n",
        "    ),\n",
        "    MLPClassifier(alpha=1, max_iter=1000, random_state=42),\n",
        "    GaussianNB(),\n",
        "]\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
        ")\n",
        "rng = np.random.RandomState(2)\n",
        "X += 2 * rng.uniform(size=X.shape)\n",
        "linearly_separable = (X, y)\n",
        "\n",
        "datasets = [\n",
        "    make_moons(noise=0.3, random_state=0),\n",
        "    make_circles(noise=0.2, factor=0.5, random_state=1),\n",
        "    linearly_separable,\n",
        "]\n",
        "\n",
        "figure = plt.figure(figsize=(27, 9))\n",
        "i = 1\n",
        "# iterate over datasets\n",
        "for ds_cnt, ds in enumerate(datasets):\n",
        "    # preprocess dataset, split into training and test part\n",
        "    X, y = ds\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.4, random_state=42\n",
        "    )\n",
        "\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "\n",
        "    # just plot the dataset first\n",
        "    cm = plt.cm.RdBu\n",
        "    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
        "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
        "    if ds_cnt == 0:\n",
        "        ax.set_title(\"Input data\")\n",
        "    # Plot the training points\n",
        "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
        "    # Plot the testing points\n",
        "    #ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\")\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    i += 1\n",
        "\n",
        "    # iterate over classifiers\n",
        "    for name, clf in zip(names, classifiers):\n",
        "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
        "\n",
        "        clf = make_pipeline(StandardScaler(), clf)\n",
        "        clf.fit(X_train, y_train)\n",
        "        score = clf.score(X_test, y_test)\n",
        "        DecisionBoundaryDisplay.from_estimator(\n",
        "            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
        "        )\n",
        "\n",
        "        # Plot the training points\n",
        "        # ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
        "        # Plot the testing points\n",
        "        ax.scatter(\n",
        "            X_test[:, 0],\n",
        "            X_test[:, 1],\n",
        "            c=y_test,\n",
        "            cmap=cm_bright,\n",
        "            edgecolors=\"k\",\n",
        "            alpha=0.6,\n",
        "        )\n",
        "\n",
        "        ax.set_xlim(x_min, x_max)\n",
        "        ax.set_ylim(y_min, y_max)\n",
        "        ax.set_xticks(())\n",
        "        ax.set_yticks(())\n",
        "        if ds_cnt == 0:\n",
        "            ax.set_title(name)\n",
        "        ax.text(\n",
        "            x_max - 0.3,\n",
        "            y_min + 0.3,\n",
        "            (\"%.2f\" % score).lstrip(\"0\"),\n",
        "            size=15,\n",
        "            horizontalalignment=\"right\",\n",
        "        )\n",
        "        i += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gI5x8fzPyrXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.2 clustering\n",
        "# Authors: The scikit-learn developers\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "import time\n",
        "import warnings\n",
        "from itertools import cycle, islice\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import cluster, datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "n_samples = 1500\n",
        "noisy_circles = datasets.make_circles(\n",
        "    n_samples=n_samples, factor=0.5, noise=0.05, random_state=170\n",
        ")\n",
        "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=170)\n",
        "blobs = datasets.make_blobs(n_samples=n_samples, random_state=170)\n",
        "rng = np.random.RandomState(170)\n",
        "no_structure = rng.rand(n_samples, 2), None\n",
        "\n",
        "# Anisotropicly distributed data\n",
        "X, y = datasets.make_blobs(n_samples=n_samples, random_state=170)\n",
        "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "X_aniso = np.dot(X, transformation)\n",
        "aniso = (X_aniso, y)\n",
        "\n",
        "# blobs with varied variances\n",
        "varied = datasets.make_blobs(\n",
        "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=170\n",
        ")\n",
        "\n",
        "# Set up cluster parameters\n",
        "plt.figure(figsize=(9 * 1.3 + 2, 14.5))\n",
        "plt.subplots_adjust(\n",
        "    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n",
        ")\n",
        "\n",
        "plot_num = 1\n",
        "\n",
        "default_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n",
        "\n",
        "datasets = [\n",
        "    (noisy_circles, {\"n_clusters\": 2}),\n",
        "    (noisy_moons, {\"n_clusters\": 2}),\n",
        "    (varied, {\"n_neighbors\": 2}),\n",
        "    (aniso, {\"n_neighbors\": 2}),\n",
        "    (blobs, {}),\n",
        "    (no_structure, {}),\n",
        "]\n",
        "\n",
        "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
        "    # update parameters with dataset-specific values\n",
        "    params = default_base.copy()\n",
        "    params.update(algo_params)\n",
        "\n",
        "    X, y = dataset\n",
        "\n",
        "    # normalize dataset for easier parameter selection\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "\n",
        "    # ============\n",
        "    # Create cluster objects\n",
        "    # ============\n",
        "    ward = cluster.AgglomerativeClustering(\n",
        "        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n",
        "    )\n",
        "    complete = cluster.AgglomerativeClustering(\n",
        "        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n",
        "    )\n",
        "    average = cluster.AgglomerativeClustering(\n",
        "        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n",
        "    )\n",
        "    single = cluster.AgglomerativeClustering(\n",
        "        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n",
        "    )\n",
        "\n",
        "    clustering_algorithms = (\n",
        "        (\"Single Linkage\", single),\n",
        "        (\"Average Linkage\", average),\n",
        "        (\"Complete Linkage\", complete),\n",
        "        (\"Ward Linkage\", ward),\n",
        "    )\n",
        "\n",
        "    for name, algorithm in clustering_algorithms:\n",
        "        t0 = time.time()\n",
        "\n",
        "        # catch warnings related to kneighbors_graph\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.filterwarnings(\n",
        "                \"ignore\",\n",
        "                message=\"the number of connected components of the \"\n",
        "                + \"connectivity matrix is [0-9]{1,2}\"\n",
        "                + \" > 1. Completing it to avoid stopping the tree early.\",\n",
        "                category=UserWarning,\n",
        "            )\n",
        "            algorithm.fit(X)\n",
        "\n",
        "        t1 = time.time()\n",
        "        if hasattr(algorithm, \"labels_\"):\n",
        "            y_pred = algorithm.labels_.astype(int)\n",
        "        else:\n",
        "            y_pred = algorithm.predict(X)\n",
        "\n",
        "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
        "        if i_dataset == 0:\n",
        "            plt.title(name, size=18)\n",
        "\n",
        "        colors = np.array(\n",
        "            list(\n",
        "                islice(\n",
        "                    cycle(\n",
        "                        [\n",
        "                            \"#377eb8\",\n",
        "                            \"#ff7f00\",\n",
        "                            \"#4daf4a\",\n",
        "                            \"#f781bf\",\n",
        "                            \"#a65628\",\n",
        "                            \"#984ea3\",\n",
        "                            \"#999999\",\n",
        "                            \"#e41a1c\",\n",
        "                            \"#dede00\",\n",
        "                        ]\n",
        "                    ),\n",
        "                    int(max(y_pred) + 1),\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
        "\n",
        "        plt.xlim(-2.5, 2.5)\n",
        "        plt.ylim(-2.5, 2.5)\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())\n",
        "        plt.text(\n",
        "            0.99,\n",
        "            0.01,\n",
        "            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n",
        "            transform=plt.gca().transAxes,\n",
        "            size=15,\n",
        "            horizontalalignment=\"right\",\n",
        "        )\n",
        "        plot_num += 1\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j65IRFfZ2mW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 5. BERT (Bidirectional Encoder Representations from Transformers)\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# 5.1 Prepare Input for BERT\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "sentence = \"Iowa State University, located in Ames, is a renowned public research university.\"\n",
        "\n",
        "# Tokenize the input sentence\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Convert tokens to input IDs\n",
        "input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
        "print(\"Input IDs:\", input_ids)\n",
        "print(\"Decoded Sentence:\", tokenizer.decode(input_ids))\n",
        "# with special tokens [CLS] and [SEP] used for classification tasks and indicating sentence boundaries\n",
        "\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "print(\"Input Tensor Keys:\", inputs.keys())  # 'input_ids' and 'attention_mask'\n",
        "print(\"Input IDs Tensor:\", inputs[\"input_ids\"])\n",
        "print(\"Attention Mask Tensor:\", inputs[\"attention_mask\"])\n",
        "# the mask indicates which tokens should be attended to (1) and which should be ignored (0)\n",
        "# In this example, all tokens have a mask value of 1, meaning all tokens should be attended to\n",
        "print()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lfql4hMJu_SA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 5.2 Get BERT Output\n",
        "\n",
        "# Pass the input through BERT\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Outputs contain 'last_hidden_state' and 'pooler_output'\n",
        "last_hidden_state = outputs.last_hidden_state\n",
        "pooled_output = outputs.pooler_output\n",
        "\n",
        "print(\"Last Hidden State Shape:\", last_hidden_state.shape)  # (batch_size, seq_len, hidden_size)\n",
        "print(\"Pooled Output Shape:\", pooled_output.shape)  # (batch_size, hidden_size)\n",
        "print()\n",
        "\n",
        "# The pooler_output can be used as a fixed-size embedding for the sentence:\n",
        "sentence_embedding = pooled_output.squeeze(0)  # Remove batch dimension\n",
        "print(\"Sentence Embedding (768-dim):\", sentence_embedding)\n",
        "print()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PCS8thzDu_SB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 5.3 Token-Level Embeddings\n",
        "\n",
        "# Extract embeddings for each token\n",
        "token_embeddings = last_hidden_state.squeeze(0)  # Remove batch dimension\n",
        "print(\"Token Embeddings Shape:\", token_embeddings.shape)  # (seq_len, hidden_size)\n",
        "\n",
        "# Example: Embedding for the first token\n",
        "print(\"First Token Embedding:\", token_embeddings[0])\n",
        "print()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-9yYOx9ku_SB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 5.4 Compute Sentence Similarity\n",
        "\n",
        "# Encode two sentences and compute their similarity\n",
        "sentence2 = \"Ames is home to Iowa State University, a prominent research institution.\"\n",
        "inputs2 = tokenizer(sentence2, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "# Get embeddings for both sentences\n",
        "outputs1 = model(**inputs)\n",
        "outputs2 = model(**inputs2)\n",
        "\n",
        "embedding1 = outputs1.pooler_output\n",
        "embedding2 = outputs2.pooler_output\n",
        "\n",
        "# Compute cosine similarity\n",
        "cosine_similarity = torch.nn.functional.cosine_similarity(embedding1, embedding2)\n",
        "print(\"Cosine Similarity between sentences:\", cosine_similarity.item())\n",
        "print()"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ZR8qov3zu_SB"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (nlp_course)",
      "language": "python",
      "name": "nlp_course"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}